[{"authors":null,"categories":null,"content":"Jinming Zhuang joined Prof. Peipei Zhou‚Äôs lab in 2021 Fall. Jinming\u0026rsquo;s research interest lies in Heterogeneous Architecture Exploration, HW/SW Co-design, Domain-Specific Accelerator Design and Programming Abstraction. Currently, Jinming is the student leader on the \u0026ldquo;Deep Learning\u0026rdquo; research thrust in the Prof. Zhou\u0026rsquo;s lab, under which the open-sourced project \u0026ldquo;CHARM\u0026rdquo; is developed and released on GitHub: https://github.com/arc-research-lab/CHARM. CHARM provides both the software compilation and hardware accelerators to compile end-to-end deep learning inference application on AMD Versal ACAP. You\u0026rsquo;re welcome to check out Jinming\u0026rsquo;s personal research website at https://www.jinmingzhuang.com/.\n","date":1730767991,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1730767991,"objectID":"01efc881200907c7f91948bb3e903b1d","permalink":"https://peipeizhou-eecs.github.io/author/jinming-zhuang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jinming-zhuang/","section":"authors","summary":"Jinming Zhuang joined Prof. Peipei Zhou‚Äôs lab in 2021 Fall. Jinming\u0026rsquo;s research interest lies in Heterogeneous Architecture Exploration, HW/SW Co-design, Domain-Specific Accelerator Design and Programming Abstraction. Currently, Jinming is the student leader on the \u0026ldquo;Deep Learning\u0026rdquo; research thrust in the Prof.","tags":null,"title":"Jinming Zhuang","type":"authors"},{"authors":null,"categories":null,"content":"Zhuoping Yang joined Prof. Peipei Zhou\u0026rsquo;s lab in 2022 Fall. Zhuoping\u0026rsquo;s research interests include academic Heterogeneous Architecture, Software and Hardware Co-design, and Programming Abstraction for Applications. Welcome to check his personal research website: https://zhuopingyang.com/.\n","date":1730767991,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1730767991,"objectID":"e47f45b6b981cbbfc6cba2768865b93d","permalink":"https://peipeizhou-eecs.github.io/author/zhuoping-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhuoping-yang/","section":"authors","summary":"Zhuoping Yang joined Prof. Peipei Zhou\u0026rsquo;s lab in 2022 Fall. Zhuoping\u0026rsquo;s research interests include academic Heterogeneous Architecture, Software and Hardware Co-design, and Programming Abstraction for Applications. Welcome to check his personal research website: https://zhuopingyang.","tags":null,"title":"Zhuoping Yang","type":"authors"},{"authors":null,"categories":null,"content":"Shixin Ji joined Prof. Peipei Zhou\u0026rsquo;s lab in 2023 Fall. Shixin\u0026rsquo;s research interests include Heterogeneous Computer Architecture, Software and Hardware Co-design.\n","date":1730767991,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1730767991,"objectID":"f639ff621489eceddbe9eea109c75b3f","permalink":"https://peipeizhou-eecs.github.io/author/shixin-ji/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shixin-ji/","section":"authors","summary":"Shixin Ji joined Prof. Peipei Zhou\u0026rsquo;s lab in 2023 Fall. Shixin\u0026rsquo;s research interests include Heterogeneous Computer Architecture, Software and Hardware Co-design.","tags":null,"title":"Shixin Ji","type":"authors"},{"authors":null,"categories":null,"content":"Xingzhen Chen joined Prof. Peipei Zhou\u0026rsquo;s lab in 2023 Fall. Xingzhen\u0026rsquo;s research interests include Robotics Applications, Software and Hardware Co-design.\n","date":1720572791,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1720572791,"objectID":"7c323407e1652e313a6e3c7cddddf952","permalink":"https://peipeizhou-eecs.github.io/author/xingzhen-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xingzhen-chen/","section":"authors","summary":"Xingzhen Chen joined Prof. Peipei Zhou\u0026rsquo;s lab in 2023 Fall. Xingzhen\u0026rsquo;s research interests include Robotics Applications, Software and Hardware Co-design.","tags":null,"title":"Xingzhen Chen","type":"authors"},{"authors":null,"categories":null,"content":"Wei Zhang joined Prof. Peipei Zhou\u0026rsquo;s lab in 2024 Fall.\n","date":1727657591,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1727657591,"objectID":"5fcee92fc1225eb4b7d91c512b0bf873","permalink":"https://peipeizhou-eecs.github.io/author/wei-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wei-zhang/","section":"authors","summary":"Wei Zhang joined Prof. Peipei Zhou\u0026rsquo;s lab in 2024 Fall.","tags":null,"title":"Wei Zhang","type":"authors"},{"authors":null,"categories":null,"content":"Sarah Schultz joined Prof. Peipei Zhou\u0026rsquo;s lab in 2024 Fall. Sarah\u0026rsquo;s research interests include Reconfigurable Computing, Software and Hardware Co-design.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f8bb423f90b6b7b1fd3d34129ab2a1fb","permalink":"https://peipeizhou-eecs.github.io/author/sarah-schultz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sarah-schultz/","section":"authors","summary":"Sarah Schultz joined Prof. Peipei Zhou\u0026rsquo;s lab in 2024 Fall. Sarah\u0026rsquo;s research interests include Reconfigurable Computing, Software and Hardware Co-design.","tags":null,"title":"Sarah Schultz","type":"authors"},{"authors":null,"categories":null,"content":"I am currently a Tenure-Track Assistant Professor at Brown University, School of Engineering. I obtained my Ph.D. in Computer Science from University of California, Los Angeles in 2019 supervised by Prof. Jason Cong, who leads UCLA VAST (VLSI Architecture, Synthesis and Technology) Group and CDSC (The Center for Domain-Specific Computing). My major interest is in Customized Computer Architecture and Programming Abstraction for Applications including Healthcare, e.g., Precision Medicine and Artificial Intelligence. I\u0026rsquo;m honored to receive \u0026ldquo;Outstanding Recognition in Research\u0026rdquo; from UCLA Samueli School of Engineering in 2019. I have also received üèÜ 2019 TCAD Donald O. Pederson Best Paper Award üèÜ in recognition of best paper published in the IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD) in the two calendar years preceding the award. My papers have also received 2023 IGSC Best Viewpoint Paper Finalist üèÜ, 2018 IEEE/ACM ICCAD Best Paper Nominee üèÜ, 2018 IEEE ISPASS Best Paper Nominee üèÜ.\nI\u0026rsquo;m actively recruiting PhD students and research interns! Self-motivated students with relevant research and project experience (compiler, GPU and FPGA programming, artificial intelligence algorithm and application development, etc.) are highly encouraged to contact me via email.  Download my CV. Website at Brown Researchers@Brown Former Website at UCLA\n","date":1730767991,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1730767991,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://peipeizhou-eecs.github.io/author/peipei-zhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/peipei-zhou/","section":"authors","summary":"I am currently a Tenure-Track Assistant Professor at Brown University, School of Engineering. I obtained my Ph.D. in Computer Science from University of California, Los Angeles in 2019 supervised by Prof.","tags":null,"title":"Peipei Zhou","type":"authors"},{"authors":null,"categories":null,"content":"Kent joined Prof. Peipei Zhou\u0026rsquo;s lab in 2023 January. Kent graduated in 2023 April and started Ph.D. in Electrical and Computer Engineering at Carnegie Mellon University. Congratulations to Kent!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d6c65411a5048b4b511d72fd42254ae1","permalink":"https://peipeizhou-eecs.github.io/author/kent-wirant/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kent-wirant/","section":"authors","summary":"Kent joined Prof. Peipei Zhou\u0026rsquo;s lab in 2023 January. Kent graduated in 2023 April and started Ph.D. in Electrical and Computer Engineering at Carnegie Mellon University. Congratulations to Kent!","tags":null,"title":"Kent Wirant","type":"authors"},{"authors":null,"categories":null,"content":"Sush joined Prof. Peipei Zhou\u0026rsquo;s lab in 2023 Fall and graduated in 2024 Spring. Sush\u0026rsquo;s research interests include computation vision, deep learning, hw/sw co-design. Sush has served as student lead for hardware acceleration in the project \u0026ldquo;Enabling Real-Time Flaw Detection in Laser Powder Bed Fusion Using In-Situ Infrared Monitoring by Integrating Image Processing, Machine Learning, and Hardware Software Co-Design\u0026rdquo; from 2023 Fall to 2024 Spring.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2f13891d2ce7771b26b54be40e8aa61d","permalink":"https://peipeizhou-eecs.github.io/author/sushmit-acharya/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sushmit-acharya/","section":"authors","summary":"Sush joined Prof. Peipei Zhou\u0026rsquo;s lab in 2023 Fall and graduated in 2024 Spring. Sush\u0026rsquo;s research interests include computation vision, deep learning, hw/sw co-design. Sush has served as student lead for hardware acceleration in the project \u0026ldquo;Enabling Real-Time Flaw Detection in Laser Powder Bed Fusion Using In-Situ Infrared Monitoring by Integrating Image Processing, Machine Learning, and Hardware Software Co-Design\u0026rdquo; from 2023 Fall to 2024 Spring.","tags":null,"title":"Sushmit Acharya","type":"authors"},{"authors":["Shixin Ji","Jinming Zhuang","Zhuoping Yang","Alex Jones","Peipei Zhou"],"categories":null,"content":"","date":1730767991,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730767991,"objectID":"5be9db868b3fb7d6b8d017eb16318471","permalink":"https://peipeizhou-eecs.github.io/publication/2024_igsc_ecofpga/","publishdate":"2024-11-05T00:53:11.299Z","relpermalink":"/publication/2024_igsc_ecofpga/","section":"publication","summary":"","tags":["IGSC"],"title":"Amortizing Embodied Carbon Across Generations (üî•üì£Best Viewpoint Paper in IGSC 2024üî•üì£! )","type":"publication"},{"authors":null,"categories":null,"content":"üì£ üì£ üì£ Thrilled and deeply honored to share that our work \u0026lsquo;Amortizing Embodied Carbon Across Generations\u0026rsquo; has been accepted by the 15th IEEE International Green and Sustainable Computing Conference (IGSC'24) and won the award of Best Viewpoint Paper! üéñüéñ üéñ\nüîç Challenge: trade-off between embodied carbon and operational carbon costs.\nüìå Baseline approach 1: keep using existing devices üëâ 0 additional embodied carbon‚úÖ, High operational carbon‚ùå\nüìå Baseline approach 2: update to new devices üëâ more embodied carbon‚ùå, Reduced operational carbon‚úÖ\nüí° Solution: We propose the ECO-FPGA system. By leveraging the chiplet technologies, ECO-FPGA can keep different generations of chiplets and gradually add newer chiplets. Our case study shows that ECO-FPGA can reduce the overall carbon cost of chips!\nMore information about ECO-FPGA:\nüëâ Author Copy Paper\nüëâ Slides\nüëâ IEEE Paper\nCongratulations to the Team: Shixin Ji, Jinming Zhuang, Zhuoping Yang, Alex K. Jones, Peipei Zhou!\n","date":1730614814,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730614814,"objectID":"4287714cd42545c8ba0fa4c5bb8cec27","permalink":"https://peipeizhou-eecs.github.io/post/2024-11-03-igsc2024/","publishdate":"2024-11-03T06:20:14.522Z","relpermalink":"/post/2024-11-03-igsc2024/","section":"post","summary":"üì£ üì£ üì£ Thrilled and deeply honored to share that our work \u0026lsquo;Amortizing Embodied Carbon Across Generations\u0026rsquo; has been accepted by the 15th IEEE International Green and Sustainable Computing Conference (IGSC'24) and won the award of Best Viewpoint Paper!","tags":null,"title":"11/03/2024 Our IEEE IGSC 2024 Paper Won Best Paper Award (Viewpoint Paper Track)!","type":"post"},{"authors":["Peiyan Dong","Jinming Zhuang","Zhuoping Yang","Shixin Ji","Yanyu Li","Dongkuan Xu","Heng Huang","Jingtong Hu","Alex Jones","Yiyu Shi","Yanzhi Wang","Peipei Zhou"],"categories":null,"content":"","date":1727743991,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1727743991,"objectID":"9f1e96c86f1dac5e3386b6b24b51b7ad","permalink":"https://peipeizhou-eecs.github.io/publication/2024_esweek_eqvit/","publishdate":"2024-10-01T00:53:11.299Z","relpermalink":"/publication/2024_esweek_eqvit/","section":"publication","summary":"","tags":["TCAD","ESWEEK"],"title":"EQ-ViT: Algorithm-Hardware Co-Design for End-to-End Acceleration of Real-Time Vision Transformer Inference on Versal ACAP Architecture (üî•üì£New Paper \u0026 Projectüî•üì£! )","type":"publication"},{"authors":["Jinming Zhuang","Jason Lau","Hanchen Ye","Zhuoping Yang","Shixin Ji","Jack Lo","Kristof Denolf","Stephen Neuendorffer","Alex K. Jones","Jingtong Hu","Deming Chen","Jason Cong","Peipei Zhou"],"categories":null,"content":"","date":1727661191,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1727661191,"objectID":"333d9f5064da9815e17b75a79392b05e","permalink":"https://peipeizhou-eecs.github.io/publication/2024_trets_charm2/","publishdate":"2024-09-30T01:53:11.299Z","relpermalink":"/publication/2024_trets_charm2/","section":"publication","summary":"","tags":["TRETS"],"title":"CHARM 2.0: Composing Heterogeneous Accelerators for Deep Learning on Versal ACAP Architecture (üî•üì£New Paper \u0026 Projectüî•üì£! )","type":"publication"},{"authors":["Yue Tang","Yukai Song","Naveena Elango","Sheena Ratnam Priya","Alex Jones","Jinjun Xiong","Peipei Zhou","Jingtong Hu"],"categories":null,"content":"","date":1727661191,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1727661191,"objectID":"5bc57b91760c8eb1c9007b22f969f535","permalink":"https://peipeizhou-eecs.github.io/publication/2024_esweek_chef/","publishdate":"2024-09-30T01:53:11.299Z","relpermalink":"/publication/2024_esweek_chef/","section":"publication","summary":"","tags":["TCAD","ESWEEK"],"title":"CHEF: A Framework for Deploying Heterogeneous Models on Clusters with Heterogeneous FPGAs (üî•üì£New Paper \u0026 Projectüî•üì£! )","type":"publication"},{"authors":["Zhuoping Yang","Wei Zhang","Shixin Ji","Peipei Zhou","Alex Jones"],"categories":null,"content":"","date":1727657591,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1727657591,"objectID":"ca2a674a98c61faf4e842042769c7bc1","permalink":"https://peipeizhou-eecs.github.io/publication/2024_esweek_ss/","publishdate":"2024-09-30T00:53:11.299Z","relpermalink":"/publication/2024_esweek_ss/","section":"publication","summary":"","tags":["ESWEEK"],"title":"Reducing Smart Phone Environmental Footprints with In-Memory Processing (üî•üì£New Paper \u0026 Projectüî•üì£! )","type":"publication"},{"authors":[],"categories":null,"content":"Welcome Sarah! Sarah will join the lab in August 2024.\nSarah graduated with Bachelor of Science in Electrical Engineering from University of Florida.\n","date":1721892152,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721892152,"objectID":"3040392495a5b0b73cd75aa84e1337db","permalink":"https://peipeizhou-eecs.github.io/post/welcome-sarah-schultz-to-join-our-lab/","publishdate":"2024-07-25T07:22:32.312Z","relpermalink":"/post/welcome-sarah-schultz-to-join-our-lab/","section":"post","summary":"Welcome Sarah! Sarah will join the lab in August 2024.\nSarah graduated with Bachelor of Science in Electrical Engineering from University of Florida.","tags":["recruiting"],"title":"2024/07/26 Welcome Sarah Schultz to join our lab!","type":"post"},{"authors":[],"categories":null,"content":"Welcome Wei! Wei will join the lab in August 2024.\nWei graduated with Bachelor of Engineering and Master of Engineering in Electronics Science and Engineering from Southern University of Science and Technology of China, Shenzhen, China.\n","date":1721892092,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721892092,"objectID":"fb7d4cf4f2e6c585abd06209245419e3","permalink":"https://peipeizhou-eecs.github.io/post/welcome-wei-zhang-to-join-our-lab/","publishdate":"2024-07-25T07:21:32.312Z","relpermalink":"/post/welcome-wei-zhang-to-join-our-lab/","section":"post","summary":"Welcome Wei! Wei will join the lab in August 2024.\nWei graduated with Bachelor of Engineering and Master of Engineering in Electronics Science and Engineering from Southern University of Science and Technology of China, Shenzhen, China.","tags":["recruiting"],"title":"2024/07/26 Welcome Wei Zhang to join our lab!","type":"post"},{"authors":["Shixin Ji","Zhuoping Yang","Xingzhen Chen","Stephen Cahoon","Jingtong Hu","Yiyu Shi","Alex Jones","Peipei Zhou"],"categories":null,"content":"","date":1720572791,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720572791,"objectID":"2d7efec4759e3649bee503b803b4fc3c","permalink":"https://peipeizhou-eecs.github.io/publication/2024_isvlsi_scarif/","publishdate":"2024-07-10T00:53:11.299Z","relpermalink":"/publication/2024_isvlsi_scarif/","section":"publication","summary":"","tags":["ISVLSI"],"title":"SCARIF: Towards Carbon Modeling of Cloud Servers with Accelerators (üî•üì£New Paper \u0026 Projectüî•üì£! )","type":"publication"},{"authors":null,"categories":null,"content":"Prof. Peipei Zhou gave an invited talk in IEEE Computer Society Annual Symposium on VLSI, ISVLSI 2024, Knoxville, Tennessee, on 07/03/2024 about SCARIF framework and paper \u0026ldquo;SCARIF: Towards Carbon Modeling of Cloud Servers with Accelerators\u0026rdquo;. SCARIF is a tool to estimate the embodied carbon emissions of data center servers with accelerator hardware (GPUs, FPGAs, ASICs, etc.) .\nYou are welcome to check the SCARIF page for the paper, open-source code, slides presentations and SCARIF tool usage demo video! My student Shixin Ji has made an excellent demo to show how to use SCARIF in Python in a single-line command.\nCode\n","date":1719987614,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719987614,"objectID":"7a56d74a286116761fc2aa39f06b59d1","permalink":"https://peipeizhou-eecs.github.io/post/2024-07-03-scarif-isvlsi2024/","publishdate":"2024-07-03T06:20:14.522Z","relpermalink":"/post/2024-07-03-scarif-isvlsi2024/","section":"post","summary":"Prof. Peipei Zhou gave an invited talk in IEEE Computer Society Annual Symposium on VLSI, ISVLSI 2024, Knoxville, Tennessee, on 07/03/2024 about SCARIF framework and paper \u0026ldquo;SCARIF: Towards Carbon Modeling of Cloud Servers with Accelerators\u0026rdquo;.","tags":null,"title":"07/03/2024 Prof. Zhou Gave an Invited Talk on SCARIF at IEEE ISVLSI 2024!","type":"post"},{"authors":["Ruiyang Qin","Jun Xia","Zhenge Jia","Meng Jiang","Ahmed Abbasi","Peipei Zhou","Jingtong Hu","Yiyu Shi"],"categories":null,"content":"","date":1719103991,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719103991,"objectID":"d9517c0fc4564ddb54969136535cdf78","permalink":"https://peipeizhou-eecs.github.io/publication/2024_dac/","publishdate":"2024-06-23T00:53:11.299Z","relpermalink":"/publication/2024_dac/","section":"publication","summary":"","tags":["DAC"],"title":"Enabling On-Device Self-Supervised LLM Personalization with Selective Synthetic Data (üî•üì£New Paper \u0026 Projectüî•üì£! )","type":"publication"},{"authors":["Jinming Zhuang","Zhuoping Yang","Shixin Ji","Heng Huang","Alex K. Jones","Jingtong Hu","Yiyu Shi","Peipei Zhou"],"categories":null,"content":"","date":1707526391,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707526391,"objectID":"7de15bb0a55b183eb5937ccc028fb071","permalink":"https://peipeizhou-eecs.github.io/publication/2024_fpga/","publishdate":"2024-02-10T00:53:11.299Z","relpermalink":"/publication/2024_fpga/","section":"publication","summary":"","tags":["FPGA"],"title":"SSR: Spatial Sequential Hybrid Architecture for Latency Throughput Tradeoff in Transformer Acceleration (üî•üì£New Paper \u0026 Projectüî•üì£! )","type":"publication"},{"authors":["Zhuoping Yang","Shixin Ji","Xingzhen Chen","Jinming Zhuang","Weifeng Zhang","Dharmesh Jani","Peipei Zhou"],"categories":null,"content":"","date":1704847991,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704847991,"objectID":"33809da743c0e6a337403284a66b98f5","permalink":"https://peipeizhou-eecs.github.io/publication/2024_aspdac/","publishdate":"2024-01-10T00:53:11.299Z","relpermalink":"/publication/2024_aspdac/","section":"publication","summary":"","tags":["ASPDAC"],"title":"Challenges and Opportunities to Enable Large-Scale Computing via Heterogeneous Chiplets (üî•üì£New Paper \u0026 Projectüî•üì£! )","type":"publication"},{"authors":null,"categories":null,"content":"Our FPGA 2024 submission, \u0026ldquo;SSR: Spatial Sequential Hybrid Architecture for Latency Throughput Tradeoff Design Space Exploration\u0026rdquo; is accepted as full paper in FPGA 2024! Congratulations to my Ph.D. students Jinming Zhuang, Zhuoping Yang, and Shixin Ji! Thanks to all my collaborators and huge thanks to the support from Pitt-ECE, AMD (software and hardware donations) and National Science Foundation! Paper preprint will be released in the near future!\n","date":1702275614,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702275614,"objectID":"bf6f93ddc8a2c7a2b836276a7cf76bef","permalink":"https://peipeizhou-eecs.github.io/post/2023-11-10-ssr-fpga2024/","publishdate":"2023-12-11T06:20:14.522Z","relpermalink":"/post/2023-11-10-ssr-fpga2024/","section":"post","summary":"Our FPGA 2024 submission, \u0026ldquo;SSR: Spatial Sequential Hybrid Architecture for Latency Throughput Tradeoff Design Space Exploration\u0026rdquo; is accepted as full paper in FPGA 2024! Congratulations to my Ph.D. students Jinming Zhuang, Zhuoping Yang, and Shixin Ji!","tags":null,"title":"12/10/2023 One Full Paper Accepted at FPGA 2024! Congratulations to Jinming, Zhuoping, and Shixin!","type":"post"},{"authors":null,"categories":null,"content":"üì£ üì£ üì£ Prof. Zhou gave an invited talk \u0026ldquo;Architectural Challenges and Innovation for Compute Infrastructure Co-Design\u0026rdquo; at OCP Global Summit 2023. You can watch the talk on YouTube now. In this talk, Prof. Zhou discussed how to use heterogeneous computing to accelerate real-time transformer inference, how to scale out computing in data centers, and how to scale up using chiplets!\nLinkedIn\n","date":1698992414,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698992414,"objectID":"8540f0c4481f25a38d5e4b7165e082a3","permalink":"https://peipeizhou-eecs.github.io/post/2023-10-18-zhou-ocp2023-talk/","publishdate":"2023-11-03T06:20:14.522Z","relpermalink":"/post/2023-10-18-zhou-ocp2023-talk/","section":"post","summary":"üì£ üì£ üì£ Prof. Zhou gave an invited talk \u0026ldquo;Architectural Challenges and Innovation for Compute Infrastructure Co-Design\u0026rdquo; at OCP Global Summit 2023. You can watch the talk on YouTube now. In this talk, Prof.","tags":null,"title":"10/18/2023 Prof. Zhou gave an invited talk at OCP Global Summit 2023!","type":"post"},{"authors":null,"categories":null,"content":"Zhuoping Yang and Prof. Peipei Zhou participated in ICCAD 2023 at San Francisco from 10/28/2023-11/02/2023.\nZhuoping has presented research paper \u0026ldquo;AIM: Accelerating Arbitrary-precision Integer Multiplication on Heterogeneous Reconfigurable Computing Platform Versal ACAP\u0026rdquo; about accelerating security-related applications on AMD Versal ACAP.\nYou are welcome to check the AIM page for the paper, open-source code and presentations.\nCode\n","date":1698906014,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698906014,"objectID":"a7a04120e795eff671d9f3721d80fe86","permalink":"https://peipeizhou-eecs.github.io/post/2023-11-02-aim-iccad2023/","publishdate":"2023-11-02T06:20:14.522Z","relpermalink":"/post/2023-11-02-aim-iccad2023/","section":"post","summary":"Zhuoping Yang and Prof. Peipei Zhou participated in ICCAD 2023 at San Francisco from 10/28/2023-11/02/2023.\nZhuoping has presented research paper \u0026ldquo;AIM: Accelerating Arbitrary-precision Integer Multiplication on Heterogeneous Reconfigurable Computing Platform Versal ACAP\u0026rdquo; about accelerating security-related applications on AMD Versal ACAP.","tags":null,"title":"11/02/2023 Zhuoping Presents 1 Research Paper at ICCAD 2023!","type":"post"},{"authors":["Peipei Zhou"],"categories":null,"content":"","date":1697630454,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697630454,"objectID":"60b85ce7b0b08b82a5711041b6205466","permalink":"https://peipeizhou-eecs.github.io/talk/architectural-challenges-and-innovation-for-compute-infrastructure-co-design/","publishdate":"2023-10-18T12:13:54.603Z","relpermalink":"/talk/architectural-challenges-and-innovation-for-compute-infrastructure-co-design/","section":"event","summary":"How to use heterogeneous computing to accelerate real-time transformer inference? How to scale out computing in data centers, and how to scale up using chiplets?","tags":null,"title":"Architectural Challenges and Innovation for Compute Infrastructure Co-Design","type":"event"},{"authors":null,"categories":null,"content":"My students Jinming Zhuang and Zhuoping Yang and I gave a remote demo \u0026ldquo;CHARM: Composing Heterogeneous AcceleRators for End-to-end Deep Learning Inference on Versal ACAP Architecture\u0026rdquo; on THE EMBEDDED SYSTEMS RESEARCH SOFTWARE COMPETITION (ESSC) in ESWEEK 2023. Thanks to the ESSC Chairs Ganapati Bhat and Biresh Kumar Joardar, ESWEEK 2023 Chairs X. Sharon Hu and Alain Girault for offering this great opportunity for us to showcase the open-source system software and artifacts and enrich open-source tools for the EDA community! In this demo, we show how to use CHARM, our open-source tool to compile deep learning inference on AMD Versal ACAP architecture (AI ASIC + FPGA + CPU). This repo has received 76 stars since we released it in Feb.\nYou are welcome to check our paper for this demo:\n  FPGA2023 CHARM: Composing Heterogeneous Accelerators for Matrix Multiply on Versal ACAP Architecture. It\u0026rsquo;s the Top 1 Downloaded paper in the FPGA 2023 proceeding.\n  DAC2023 High Performance, Low Power Matrix Multiply Design on ACAP: from Architecture, Design Challenges and DSE Perspectives\n  Thanks to our collaborators in this project, Prof. Jingtong Hu, Prof. Alex Jones, Prof. Deming Chen, Prof. Jason Cong, student collaborators Hanchen Ye, Jason Lau, and our AMD collaborators Stephen Neuendorffer, Jack Lo, and Kristof Denolf!\nLinkedIn\n","date":1695277214,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695277214,"objectID":"927262a2122b0d516d2822ff75534773","permalink":"https://peipeizhou-eecs.github.io/post/2023-09-19-charm-demo-esweek2023-essc/","publishdate":"2023-09-21T06:20:14.522Z","relpermalink":"/post/2023-09-19-charm-demo-esweek2023-essc/","section":"post","summary":"My students Jinming Zhuang and Zhuoping Yang and I gave a remote demo \u0026ldquo;CHARM: Composing Heterogeneous AcceleRators for End-to-end Deep Learning Inference on Versal ACAP Architecture\u0026rdquo; on THE EMBEDDED SYSTEMS RESEARCH SOFTWARE COMPETITION (ESSC) in ESWEEK 2023.","tags":null,"title":"09/19/2023 CHARM Demo at ESWEEK2023 ESSC!","type":"post"},{"authors":[],"categories":null,"content":"Welcome Shixin! Shixin joined the lab in August 2023.\nShixin graduated with Bachelor of Science in Electronic and Information Engineering in Qian Xuesen Class (Honor Program) of Xi\u0026rsquo;an Jiaotong University.\n","date":1694071292,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694071292,"objectID":"9e0268618057ad371e9695b962192fec","permalink":"https://peipeizhou-eecs.github.io/post/welcome-shixin-ji-to-join-our-lab/","publishdate":"2023-09-07T07:21:32.312Z","relpermalink":"/post/welcome-shixin-ji-to-join-our-lab/","section":"post","summary":"Welcome Shixin! Shixin joined the lab in August 2023.\nShixin graduated with Bachelor of Science in Electronic and Information Engineering in Qian Xuesen Class (Honor Program) of Xi\u0026rsquo;an Jiaotong University.","tags":["recruiting"],"title":"2023/09/07 Welcome Shixin Ji to join our lab!","type":"post"},{"authors":[],"categories":null,"content":"Welcome Sush! Sush joined the lab as an undergraduate student researcher in August 2023.\nSush is currently a senior student in Pitt ECE department.\n","date":1694071292,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694071292,"objectID":"45826a7913ea369f3f4674c24c470d39","permalink":"https://peipeizhou-eecs.github.io/post/welcome-sushmit-acharya-to-join-our-lab/","publishdate":"2023-09-07T07:21:32.312Z","relpermalink":"/post/welcome-sushmit-acharya-to-join-our-lab/","section":"post","summary":"Welcome Sush! Sush joined the lab as an undergraduate student researcher in August 2023.\nSush is currently a senior student in Pitt ECE department.","tags":["recruiting"],"title":"2023/09/07 Welcome Sushmit Acharya to join our lab!","type":"post"},{"authors":[],"categories":null,"content":"Welcome Xingzhen! Xingzhen joined the lab in August 2023.\nXingzhen graduated with Bachelor of Science in Electrical Engineering and Automation in 2022 from Huazhong University of Science and Technology.\n","date":1694071292,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694071292,"objectID":"97a69a211fc10160c5f83c59dcbc28e5","permalink":"https://peipeizhou-eecs.github.io/post/welcome-xingzhen-chen-to-join-our-lab/","publishdate":"2023-09-07T07:21:32.312Z","relpermalink":"/post/welcome-xingzhen-chen-to-join-our-lab/","section":"post","summary":"Welcome Xingzhen! Xingzhen joined the lab in August 2023.\nXingzhen graduated with Bachelor of Science in Electrical Engineering and Automation in 2022 from Huazhong University of Science and Technology.","tags":["recruiting"],"title":"2023/09/07 Welcome Xingzhen Chen to join our lab!","type":"post"},{"authors":null,"categories":null,"content":"Jinming Zhuang and Prof. Peipei Zhou participated in DAC'60 at San Francisco from 07/09/2023-07/14/2023.\nJinming has presented research paper \u0026ldquo;High Performance, Low Power Matrix Multiply Design on ACAP: from Architecture, Design Challenges and DSE Perspectives\u0026rdquo; and 1 demo at the ACM SIGDA/IEEE CEDA University Demonstration about real-time vision transformer inference on AMD Versal ACAP.\n","date":1689920414,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689920414,"objectID":"8776fe987e34ad4532fa012b4d1da965","permalink":"https://peipeizhou-eecs.github.io/post/07-14-2023-jinming-presents-1-research-paper-and-1-university-demo-at-dac60/","publishdate":"2023-07-21T06:20:14.522Z","relpermalink":"/post/07-14-2023-jinming-presents-1-research-paper-and-1-university-demo-at-dac60/","section":"post","summary":"Jinming Zhuang and Prof. Peipei Zhou participated in DAC'60 at San Francisco from 07/09/2023-07/14/2023.\nJinming has presented research paper \u0026ldquo;High Performance, Low Power Matrix Multiply Design on ACAP: from Architecture, Design Challenges and DSE Perspectives\u0026rdquo; and 1 demo at the ACM SIGDA/IEEE CEDA University Demonstration about real-time vision transformer inference on AMD Versal ACAP.","tags":null,"title":"07/14/2023 Jinming Presents 1 Research Paper and 1 University Demo at DAC60!","type":"post"},{"authors":null,"categories":null,"content":"Our ICCAD 2023 submission, ‚ÄúAIM: Accelerating Arbitrary-precision Integer Multiplication on Heterogeneous Reconfigurable Computing Platform Versal ACAP ‚Äù is accepted in ICCAD 2023! Congratulations to my Ph.D. students Zhuoping Yang and Jinming, especially Zhuoping, on his remarkable achievement in getting his first 1st-author paper in the first year of Ph.D. study. Thanks to all my collaborators and huge thanks to the support from Pitt-ECE, AMD and National Science Foundation!\n","date":1689917150,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689917150,"objectID":"e91cbffe59ccd7ff1a031250f5cf0695","permalink":"https://peipeizhou-eecs.github.io/post/07-20-2023-our-paper-accepted-to-iccad-2023/","publishdate":"2023-07-21T05:25:50.758Z","relpermalink":"/post/07-20-2023-our-paper-accepted-to-iccad-2023/","section":"post","summary":"Our ICCAD 2023 submission, ‚ÄúAIM: Accelerating Arbitrary-precision Integer Multiplication on Heterogeneous Reconfigurable Computing Platform Versal ACAP ‚Äù is accepted in ICCAD 2023! Congratulations to my Ph.D. students Zhuoping Yang and Jinming, especially Zhuoping, on his remarkable achievement in getting his first 1st-author paper in the first year of Ph.","tags":null,"title":"07/20/2023 Our Paper Accepted to ICCAD 2023!","type":"post"},{"authors":["Zhuoping Yang","Jinming Zhuang","Jiaqi Yin","Cunxi Yu","Alex K. Jones","Peipei Zhou"],"categories":null,"content":"","date":1689814391,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689814391,"objectID":"20b81b334929a31f64f4140f27552667","permalink":"https://peipeizhou-eecs.github.io/publication/2023iccad/","publishdate":"2023-07-20T00:53:11.299Z","relpermalink":"/publication/2023iccad/","section":"publication","summary":"","tags":["ICCAD"],"title":"AIM: Accelerating Arbitrary-precision Integer Multiplication on Heterogeneous Reconfigurable Computing Platform Versal ACAP (üî•üì£New Paper \u0026 Projectüî•üì£! )","type":"publication"},{"authors":null,"categories":null,"content":"What? üéâ\nThe 3rd International HiPChips Workshop, along with ISCA-2023\nOnline Attend? üåê Virtual (Yes, It is Free to Everyone) !!\nZoom Link: https://us06web.zoom.us/j/83130309538\nWhen? ‚è∞\nDay-Date: Saturday, June 17th, 2023, 9AM - 5PM EST, Whole-Day Event\nWhere? üìç\nLocation: Orlando, Florida, Marriott World Center Orlando\nVenue: Please visit the ISCA-2023 website or go directly to the HiPChips website at: https://hipchips.github.io/isca2023/#schedule\n","date":1686808440,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686808440,"objectID":"a5a459117eb439f64969b3eee6a6277b","permalink":"https://peipeizhou-eecs.github.io/post/06-17-2023-prof-zhou-will-organize-hipchips-2023/","publishdate":"2023-06-15T05:54:00.269Z","relpermalink":"/post/06-17-2023-prof-zhou-will-organize-hipchips-2023/","section":"post","summary":"What? üéâ\nThe 3rd International HiPChips Workshop, along with ISCA-2023\nOnline Attend? üåê Virtual (Yes, It is Free to Everyone) !!\nZoom Link: https://us06web.zoom.us/j/83130309538\nWhen? ‚è∞\nDay-Date: Saturday, June 17th, 2023, 9AM - 5PM EST, Whole-Day Event","tags":null,"title":"06/17/2023 Prof. Zhou Will Organize HipChips2023 Co-located at ISCA2023 with DJ and Weifeng","type":"post"},{"authors":null,"categories":null,"content":"Prof. Zhou was invited to CMU CrossRoads Seminar (04/07/2023) and TAMU CESG Seminar to present \u0026ldquo;CHARM: Composing Heterogeneous Accelerators for Matrix Multiply on Versal ACAP Architecture\u0026rdquo;. Check it out on the üî•üì£ recorded video: YouTubeüî•üì£\n","date":1686806529,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686806529,"objectID":"dd44e1c5ba50f340d2a68568d8cf4df1","permalink":"https://peipeizhou-eecs.github.io/post/04-21-prof-zhou-invited-talks/","publishdate":"2023-06-15T05:22:09.811Z","relpermalink":"/post/04-21-prof-zhou-invited-talks/","section":"post","summary":"Prof. Zhou was invited to CMU CrossRoads Seminar (04/07/2023) and TAMU CESG Seminar to present \u0026ldquo;CHARM: Composing Heterogeneous Accelerators for Matrix Multiply on Versal ACAP Architecture\u0026rdquo;. Check it out on the üî•üì£ recorded video: YouTubeüî•üì£","tags":null,"title":"04/21/2023 Prof. Zhou Invited Talks (Watch Recorded Video on YouTube!)","type":"post"},{"authors":["Peipei Zhou"],"categories":null,"content":"","date":1680869634,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680869634,"objectID":"c87c9f814c2ec1d0bf131d1e9771e0e7","permalink":"https://peipeizhou-eecs.github.io/talk/charm-composing-heterogeneous-accelerators-for-matrix-multiply-on-versal-acap-architecture/","publishdate":"2023-06-14T12:13:54.603Z","relpermalink":"/talk/charm-composing-heterogeneous-accelerators-for-matrix-multiply-on-versal-acap-architecture/","section":"event","summary":"Which platform beats 7nm GPU A100 in energy efficiency? AMD Versal ACAP (FPGA+AI Chip) How to program AMD Versal ACAP, i.e., FPGA + AI Chip within the same chip die for deep learning applications in 10 lines of code? Use CHARM","tags":null,"title":"CHARM Composing Heterogeneous Accelerators for Matrix Multiply on Versal ACAP Architecture","type":"event"},{"authors":["Jinming Zhuang","Zhuoping Yang","Peipei Zhou"],"categories":null,"content":"","date":1677199991,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677199991,"objectID":"62ce451cddb0ece190f3b55c21cbb564","permalink":"https://peipeizhou-eecs.github.io/publication/2023dac/","publishdate":"2023-02-24T00:53:11.299Z","relpermalink":"/publication/2023dac/","section":"publication","summary":"","tags":["DAC"],"title":"High Performance, Low Power Matrix Multiply Design on ACAP: from Architecture, Design Challenges and DSE Perspectives (üî•üì£New Paper \u0026 Projectüî•üì£! )","type":"publication"},{"authors":null,"categories":null,"content":"Our¬†DAC¬†2023 submission,¬†\u0026ldquo;High Performance, Low Power Matrix Multiply Design on ACAP: from Architecture, Design Challenges and DSE Perspectives\u0026rdquo; is accepted in¬†DAC 2023! Congratulations to my PhD students Jinming Zhuang and Zhuoping Yang!\n","date":1676316821,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676316821,"objectID":"a4f5c22162bd6d64f54076905a16b9a8","permalink":"https://peipeizhou-eecs.github.io/post/02-13-2023-our-paper-accepted-to-dac-2023/","publishdate":"2023-02-13T19:33:41.341Z","relpermalink":"/post/02-13-2023-our-paper-accepted-to-dac-2023/","section":"post","summary":"Our¬†DAC¬†2023 submission,¬†\u0026ldquo;High Performance, Low Power Matrix Multiply Design on ACAP: from Architecture, Design Challenges and DSE Perspectives\u0026rdquo; is accepted in¬†DAC 2023! Congratulations to my PhD students Jinming Zhuang and Zhuoping Yang!","tags":null,"title":"02/13/2023 Our Paper Accepted to DAC 2023!","type":"post"},{"authors":["Jinming Zhuang","Jason Lau","Hanchen Ye","Zhuoping Yang","Yubo Du","Jack Lo","Kristof Denolf","Stephen Neuendorffer","Alex K. Jones","Jingtong Hu","Deming Chen","Jason Cong","Peipei Zhou"],"categories":null,"content":"","date":1672534391,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672534391,"objectID":"f704c7ce18051c6e76da0a657cecdcda","permalink":"https://peipeizhou-eecs.github.io/publication/fpga23/","publishdate":"2023-01-01T00:53:11.299Z","relpermalink":"/publication/fpga23/","section":"publication","summary":"Dense matrix multiply (MM) serves as one of the most heavily used kernels in deep learning applications. To cope with the high computation demands of these applications, heterogeneous architectures featuring both FPGA and dedicated ASIC accelerators have emerged as promising platforms. For example, the AMD/Xilinx Versal ACAP architecture combines general-purpose CPU cores and programmable logic (PL) with AI Engine processors (AIE) optimized for AI/ML.  An array of 400 AI Engine processors executing at 1 GHz can theoretically provide up to 6.4 TFLOPs performance for 32-bit floating-point (fp32) data. However, machine learning models often contain both large and small MM operations.  While large MM operations can be parallelized efficiently across many cores, small MM operations typically cannot. In our investigation, we observe that executing some small MM layers from the BERT natural language processing model on a large, monolithic MM accelerator in Versal ACAP achieved less than 5% of the theoretical peak performance. Therefore, one key question arises, how can we design accelerators to fully use the abundant computation resources under limited communication bandwidth for end-to-end applications with multiple MM layers of diverse sizes? We identify the biggest system throughput bottleneck resulting from the mismatch of massive computation resources of one monolithic accelerator and the various MM layers of small sizes in the application. To resolve this problem, we propose the CHARM framework to compose multiple diverse MM accelerator architectures working concurrently towards different layers within one application. CHARM includes analytical models which guide design space exploration to determine accelerator partitions and layer scheduling. To facilitate the system designs, CHARM automatically generates code, enabling thorough onboard design verification. We deploy the CHARM framework for four different deep learning applications, including BERT, ViT, NCF, MLP, on the AMD/Xilinx Versal ACAP VCK190 evaluation board. Our experiments show that we achieve 1.46 TFLOPs, 1.61 TFLOPs, 1.74 TFLOPs, and 2.94 TFLOPs inference throughput for BERT, ViT, NCF, MLP, respectively, which obtain 5.40x, 32.51x, 1.00x and 1.00x throughput gains compared to one monolithic accelerator.","tags":["FPGA"],"title":"CHARM: Composing Heterogeneous AcceleRators for Matrix Multiply on Versal ACAP Architecture (üî•üì£New Paper \u0026 Projectüî•üì£! )","type":"publication"},{"authors":["S√©bastien Ollivier","Sheng Li","Yue Tang","Chayanika Chaudhuri","Peipei Zhou","Xulong Tang","Jingtong Hu","Alex K. Jones"],"categories":null,"content":"","date":1671441152,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671441152,"objectID":"ad6ffc9478391c6507624f41a46afc5a","permalink":"https://peipeizhou-eecs.github.io/publication/2022_ieee_micro/","publishdate":"2022-12-19T09:12:32.506Z","relpermalink":"/publication/2022_ieee_micro/","section":"publication","summary":"","tags":["IEEE Micro"],"title":"Sustainable AI Processing at the Edge","type":"publication"},{"authors":null,"categories":null,"content":"Our¬†FPGA¬†2023 submission,¬†CHARM: Composing Heterogeneous Accelerators for Matrix Multiply on Versal ACAP Architecture¬†is accepted in¬†FPGA¬†2023! Congratulations to my PhD student Jinming Zhuang on the first-author publication! TÔªøhis is a collaborated work with UIUC, UCLA, AMD/Xilinx. Huge thanks to all the collaborators!\nTÔªøhe preprint was released in Publication! The open-source project on GitHub ARC Research Lab! You are welcome to read and send us the feedbacks!\n","date":1670908938,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670908938,"objectID":"dc360ed61194c507449f08dc18005e4f","permalink":"https://peipeizhou-eecs.github.io/post/fpga23-paper-accepted/","publishdate":"2022-12-13T05:22:18.139Z","relpermalink":"/post/fpga23-paper-accepted/","section":"post","summary":"Our¬†FPGA¬†2023 submission,¬†CHARM: Composing Heterogeneous Accelerators for Matrix Multiply on Versal ACAP Architecture¬†is accepted in¬†FPGA¬†2023! Congratulations to my PhD student Jinming Zhuang on the first-author publication!","tags":null,"title":"FPGA23 Paper Accepted!!","type":"post"},{"authors":null,"categories":null,"content":"Prof. Zhou will attend DAC 2022 taking place this July 10-14 at the Moscone Center West in San Francisco, CA. Prof. Zhou will serve as the¬†Session Chair¬†for¬†EDA System-on-Chip Design Methodology \u0026ldquo;Fantastic SoCs and Where to Find Them!\u0026rdquo; on Tuesday July 12th 1:30pm to 3pm PST at¬†3007. Prof. Zhou will also serve as vice co-chair for University Demonstration at DAC 2022. University¬†Demo¬†will be held on¬†Tuesday July 12th 7pm to 9pm PST at¬†Level 3 Lobby.\nlink: https://59dac.conference-program.com/\n","date":1657267055,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657267055,"objectID":"2276e06b3bba8eec6ba6136fff4d1a9e","permalink":"https://peipeizhou-eecs.github.io/post/2022-07-08-prof-zhou-will-serve-as-session-chair-for-dac-2022-eda-system-on-chip-design-methodology-san-francisco/","publishdate":"2022-07-08T07:57:35.732Z","relpermalink":"/post/2022-07-08-prof-zhou-will-serve-as-session-chair-for-dac-2022-eda-system-on-chip-design-methodology-san-francisco/","section":"post","summary":"Prof. Zhou will attend DAC 2022 taking place this July 10-14 at the Moscone Center West in San Francisco, CA. Prof. Zhou will serve as the¬†Session Chair¬†for¬†EDA System-on-Chip Design Methodology \u0026ldquo;Fantastic SoCs and Where to Find Them!","tags":["conference"],"title":"2022/07/08 Prof. Zhou will serve as session chair for DAC 2022 EDA System-on-Chip Design Methodology, San Francisco","type":"post"},{"authors":null,"categories":null,"content":"Prof. Zhou will serve as TPC for 2022 International Conference on Computer-Aided Design, 2023 Design, Automation and Test in Europe Conference | The European Event for Electronic System Design \u0026amp; Test , 2023 IEEE/ACM Conference on Connected Health Applications, Systems, and Engineering Technologies (CHASE)\n","date":1657265804,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657265804,"objectID":"6885664022fe4b52bb9779ce23df4538","permalink":"https://peipeizhou-eecs.github.io/post/prof-zhou-will-serve-as-technical-program-committee-tpc-for-2022-iccad-2023-date-2023-chase/","publishdate":"2022-07-08T07:36:44.473Z","relpermalink":"/post/prof-zhou-will-serve-as-technical-program-committee-tpc-for-2022-iccad-2023-date-2023-chase/","section":"post","summary":"Prof. Zhou will serve as TPC for 2022 International Conference on Computer-Aided Design, 2023 Design, Automation and Test in Europe Conference | The European Event for Electronic System Design \u0026amp; Test , 2023 IEEE/ACM Conference on Connected Health Applications, Systems, and Engineering Technologies (CHASE)","tags":null,"title":"2022/07/08 Prof. Zhou will serve as technical program committee (TPC) for 2022 ICCAD, 2023 DATE, 2023 CHASE","type":"post"},{"authors":[],"categories":null,"content":"Welcome Zhuoping! Zhuoping will join the lab in August 2022.\nZhuoping graduated with Bachelor of Engineering in Electronic Packaging and Master of Engineering in Electronic Packaging from Huazhong University of Science and Technology.\n","date":1657264892,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657264892,"objectID":"4ab1ea3f108cf3cc8504583ffb3a7d72","permalink":"https://peipeizhou-eecs.github.io/post/welcome-zhuoping-yang-to-join-our-lab/","publishdate":"2022-07-08T07:21:32.312Z","relpermalink":"/post/welcome-zhuoping-yang-to-join-our-lab/","section":"post","summary":"Welcome Zhuoping! Zhuoping will join the lab in August 2022.\nZhuoping graduated with Bachelor of Engineering in Electronic Packaging and Master of Engineering in Electronic Packaging from Huazhong University of Science and Technology.","tags":["recruiting"],"title":"2022/07/08 Welcome Zhuoping Yang to join our lab!","type":"post"},{"authors":["Xinyi Zhang","Cong Hao","Peipei Zhou","Alex Jones","Jingtong Hu"],"categories":null,"content":"","date":1646449993,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646449993,"objectID":"28385e95cf8f298676c5f80035721cee","permalink":"https://peipeizhou-eecs.github.io/publication/2022_dac/","publishdate":"2022-03-05T03:13:13.544Z","relpermalink":"/publication/2022_dac/","section":"publication","summary":"The complex nature of real-world problems calls for heterogeneity in both machine learning (ML) models and hardware systems. For the algorithm, the heterogeneity in ML models comes from the multi-sensor perceiving and multi-task learning, i.e., multi-modality multi-task (MMMT) models, resulting in diverse deep neural net- work (DNN) layers and computation patterns. For the system, it becomes prevailing to integrate dedicated acceleration components into one system. It thus introduces a new problem, heterogeneous model to heterogeneous system mapping (H2H), in which both computation and communication efficiency need to be considered. While previous mapping algorithms only focus on computation patterns, in this work, we propose a novel mapping algorithm with both computation and communication awareness. By slightly sacrificing computation efficiency, the communication latency is largely reduced. Therefore, the system overall performance is improved and energy is also reduced. The superior performance of our work is evaluated on MAESTRO, achieving 15%-74% latency improvement and 23%-64% energy reduction when compared with the existing computation-prioritized mapping algorithm.","tags":["DAC"],"title":"H2H: Heterogeneous Model to Heterogeneous System Mapping with Computation and Communication Awareness","type":"publication"},{"authors":["Yue Tang","Xinyi Zhang","Peipei Zhou","Jingtong Hu"],"categories":null,"content":"","date":1645776277,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645776277,"objectID":"53c9ee3a10d5e893a328b9a4d76e2f05","permalink":"https://peipeizhou-eecs.github.io/publication/2022_todaes/","publishdate":"2022-02-25T08:04:37.024Z","relpermalink":"/publication/2022_todaes/","section":"publication","summary":"Conventionally, DNN models are trained once in the cloud and deployed in edge devices such as cars, robots, or unmanned aerial vehicles (UAVs) for real-time inference. However, there are many cases that require the models to adapt to new environments, domains, or new users. In order to realize such domain adaption or personalization, the models on devices need to be continuously trained on the device. In this work, we design EF-Train, an efficient DNN training accelerator with a unified channel-level parallelism-based convolution kernel that can achieve end-to-end training on resource-limited low-power edge-level FPGAs. It is challenging to implement on-device training on resource-limited FPGAs due to the low efficiency caused by different memory access patterns among forward, backward propagation, and weight update. Therefore, we developed a data reshaping approach with intra-tile continuous memory allocation and weight reuse. An analytical model is established to automatically schedule computation and memory resources to achieve high energy efficiency on edge FPGAs. The experimental results show that our design achieves 46.99 GFLOPS and 6.09GFLOPS/W in terms of throughput and energy efficiency, respectively.","tags":["TODAES"],"title":"EF-Train: Enable Efficient On-device CNN Training on FPGA Through Data Reshaping for Online Adaptation or Personalization","type":"publication"},{"authors":null,"categories":null,"content":"One paper \u0026ldquo;EF-Train: Enable Efficient On-device CNN Training on FPGA Through Data Reshaping for Online Adaptation or Personalization\u0026rdquo; got accepted in ACM Transactions on Design Automation of Embedded Systems (TODAES) Special Issue on Energy-Efficient AI Chips\n","date":1645170217,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645170217,"objectID":"9177697ac2258e99411943ef0748411f","permalink":"https://peipeizhou-eecs.github.io/post/one-paper-accepted-in-todaes/","publishdate":"2022-02-18T07:43:37.768Z","relpermalink":"/post/one-paper-accepted-in-todaes/","section":"post","summary":"One paper \u0026ldquo;EF-Train: Enable Efficient On-device CNN Training on FPGA Through Data Reshaping for Online Adaptation or Personalization\u0026rdquo; got accepted in ACM Transactions on Design Automation of Embedded Systems (TODAES) Special Issue on Energy-Efficient AI Chips","tags":null,"title":"One Paper Accepted in TODAES","type":"post"},{"authors":null,"categories":null,"content":"One paper ‚ÄúH2H: Heterogeneous Model to Heterogeneous System Mapping with Computation and Communication Awareness‚Äù (Xinyi Zhang et al.) got accepted by¬†DAC‚Äô22.\n","date":1645170110,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645170110,"objectID":"b66e316eedf92340441258994ac66a49","permalink":"https://peipeizhou-eecs.github.io/post/one-paper-accepted-in-dac22/","publishdate":"2022-02-18T07:41:50.534Z","relpermalink":"/post/one-paper-accepted-in-dac22/","section":"post","summary":"One paper ‚ÄúH2H: Heterogeneous Model to Heterogeneous System Mapping with Computation and Communication Awareness‚Äù (Xinyi Zhang et al.) got accepted by¬†DAC‚Äô22.","tags":null,"title":"One Paper Accepted in DAC'22","type":"post"},{"authors":null,"categories":null,"content":"Welcome Jinming join our lab! Jinming joined the lab in September 2021.\n","date":1638424782,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638424782,"objectID":"ee699d144c3f84348514aaad89f34aaf","permalink":"https://peipeizhou-eecs.github.io/post/welcome-jinming-zhuang-to-join-our-lab/","publishdate":"2021-12-02T05:59:42.458Z","relpermalink":"/post/welcome-jinming-zhuang-to-join-our-lab/","section":"post","summary":"Welcome Jinming join our lab! Jinming joined the lab in September 2021.","tags":["recruiting"],"title":"Welcome Jinming Zhuang join our lab!","type":"post"},{"authors":null,"categories":null,"content":"Dr. Zhou will serve as session chair for Session: Data Collection and Analysis at IEEE/ACM Conference on Connected Health Applications, Systems, and Engineering Technologies (CHASE 2021), December 16 - 18, 2021 Washington D.C., USA.\nhttps://conferences.computer.org/chase2021/program.html\n","date":1637731481,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637731481,"objectID":"11e9510c76099c62307693821cab3507","permalink":"https://peipeizhou-eecs.github.io/post/dr-zhou-will-serve-as-session-chair-for-chase-2021/","publishdate":"2021-11-24T05:24:41.06Z","relpermalink":"/post/dr-zhou-will-serve-as-session-chair-for-chase-2021/","section":"post","summary":"Dr. Zhou will serve as session chair for Session: Data Collection and Analysis at IEEE/ACM Conference on Connected Health Applications, Systems, and Engineering Technologies (CHASE 2021), December 16 - 18, 2021 Washington D.","tags":null,"title":"11/24/2021 Dr. Zhou will serve as session chair for CHASE 2021","type":"post"},{"authors":null,"categories":null,"content":"Dr. Zhou will serve on TPC for Fifth Conference on Machine Learning and Systems (MLSys 2022), Apr. 11th to 14th, 2022 at Santa Clara Convention Center.\n","date":1637730428,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637730428,"objectID":"089afd8baf23822037083b2e878c2a2e","permalink":"https://peipeizhou-eecs.github.io/post/11-24-2021-dr-zhou-will-serve-on-tpc-for-mlsys-2022/","publishdate":"2021-11-24T05:07:08.178Z","relpermalink":"/post/11-24-2021-dr-zhou-will-serve-on-tpc-for-mlsys-2022/","section":"post","summary":"Dr. Zhou will serve on TPC for Fifth Conference on Machine Learning and Systems (MLSys 2022), Apr. 11th to 14th, 2022 at Santa Clara Convention Center.","tags":null,"title":"11/24/2021 Dr. Zhou will serve on TPC for MLSys 2022","type":"post"},{"authors":null,"categories":null,"content":"Dr. Zhou is invited to serve on Technique Program Committee for The 30th IEEE International Symposium On Field-Programmable Custom Computing Machines (FCCM 2022), May 15 ‚Äî May 18, 2022, New York.\nCFP: https://www.fccm.org/call-for-papers/.\n","date":1637730097,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637730097,"objectID":"fc420df3a42f47d5fd440a2913a52540","permalink":"https://peipeizhou-eecs.github.io/post/11-23-2021-dr-zhou-will-serve-on-tpc-for-fccm-2022/","publishdate":"2021-11-24T05:01:37.29Z","relpermalink":"/post/11-23-2021-dr-zhou-will-serve-on-tpc-for-fccm-2022/","section":"post","summary":"Dr. Zhou is invited to serve on Technique Program Committee for The 30th IEEE International Symposium On Field-Programmable Custom Computing Machines (FCCM 2022), May 15 ‚Äî May 18, 2022, New York.","tags":null,"title":"11/23/2021 Dr. Zhou will serve on TPC for FCCM 2022","type":"post"},{"authors":null,"categories":null,"content":"Dr. Zhou will attend DAC 2021 Dec 5th to Dec 9th 2021 and serve as the Session Chair for EDA System-on-Chip Design Methodology \u0026ldquo;Fantastic SoCs and Where to Find Them!\u0026quot;\n","date":1637729246,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637729246,"objectID":"cff2647fd7c9b361e64fd4139001b79a","permalink":"https://peipeizhou-eecs.github.io/post/peipei-zhou-will-serve-as-session-chair-for-dac-2021-eda-system-on-chip-design/","publishdate":"2021-11-24T04:47:26.304Z","relpermalink":"/post/peipei-zhou-will-serve-as-session-chair-for-dac-2021-eda-system-on-chip-design/","section":"post","summary":"Dr. Zhou will attend DAC 2021 Dec 5th to Dec 9th 2021 and serve as the Session Chair for EDA System-on-Chip Design Methodology \u0026ldquo;Fantastic SoCs and Where to Find Them!\u0026quot;","tags":null,"title":"11/22/2021 Peipei Zhou will serve as session chair for DAC 2021: EDA System-on-Chip Design","type":"post"},{"authors":["Xinyi Zhang","Yawen Wu","Peipei Zhou","Xulong Tang","Jingtong Hu"],"categories":null,"content":"","date":1632933408,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632933408,"objectID":"f8efb5979ddb735398ec397e6655d874","permalink":"https://peipeizhou-eecs.github.io/publication/tecs2021/","publishdate":"2021-09-29T16:36:48.458Z","relpermalink":"/publication/tecs2021/","section":"publication","summary":"Multi-head self-attention (attention mechanism) has been employed in a variety of fields such as machine translation, language modeling, and image processing due to its superiority in feature extraction and sequential data analysis. This is benefited from a large number of parameters and sophisticated model architecture behind the attention mechanism. To efficiently deploy attention mechanism on resource-constrained devices, existing works propose to reduce the model size by building a customized smaller model or compressing a big standard model. A customized smaller model is usually optimized for the specific task and needs effort in model parameters exploration. Model compression reduces model size without hurting the model architecture robustness, which can be efficiently applied to different tasks. The compressed weights in the model are usually regularly shaped (e.g. rectangle) but the dimension sizes vary (e.g. differs in rectangle height and width). Such compressed attention mechanism can be efficiently deployed on CPU/GPU platforms as their memory and computing resources can be flexibly assigned with demand. However, for Field Programmable Gate Arrays (FPGAs), the data buffer allocation and computing kernel are fixed at run time to achieve maximum energy efficiency. After compression, weights are much smaller and different in size, which leads to inefficient utilization of FPGA on-chip buffer. Moreover, the different weight heights and widths may lead to inefficient FPGA computing kernel execution. Due to the large number of weights in the attention mechanism, building a unique buffer and computing kernel for each compressed weight on FPGA is not feasible. In this work, we jointly consider the compression impact on buffer allocation and the required computing kernel during the attention mechanism compressing. A novel structural pruning method with memory footprint awareness is proposed and the associated accelerator on FPGA is designed. The experimental results show that our work can compress Transformer (an attention mechanism based model) by 95x. The developed accelerator can fully utilize the FPGA resource, processing the sparse attention mechanism with the run-time throughput performance of 1.87 Tops in ZCU102 FPGA.","tags":["TECS","ESWEEK"],"title":"Algorithm-hardware Co-design of Attention Mechanism on FPGA Devices","type":"publication"},{"authors":["Peipei Zhou","Jiayi Sheng","Cody Hao Yu","Peng Wei","Jie Wang","Di Wu","Jason Cong"],"categories":null,"content":"","date":1620984449,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620984449,"objectID":"6fe94c89edf32709a2c92e29b56e496f","permalink":"https://peipeizhou-eecs.github.io/publication/mocha/","publishdate":"2021-05-14T09:27:29.087Z","relpermalink":"/publication/mocha/","section":"publication","summary":"FPGAs have been widely deployed in public clouds, e.g., Amazon Web Services (AWS) and Huawei Cloud. However, simply offloading accelerated kernels from CPU hosts to PCIe-based FPGAs does not guarantee out-of-pocket cost savings in a pay-as-you-go public cloud. Taking Genome Analysis Toolkit (GATK) applications as case studies, although the adoption of FPGAs reduces the overall execution time, it introduces 2.56√ó extra cost, due to insufficient application-level speedup by Amdahl‚Äôs law. To optimize the out-of-pocket cost while keeping high speedup and throughput, we propose Mocha framework as a distributed runtime system to fully utilize the accelerator resource by accelerator sharing and CPU-FPGA partial task offloading. Evaluation results on HaplotypeCaller (HTC) and Mutect2 in GATK show that on AWS, Mocha saves on the application cost by 2.82x for HTC, 1.06x for Mutect2 and on Huawei Cloud by 1.22x, 1.52x respectively than straightforward CPU-FPGA integration solution with less than 5.1% performance overhead.","tags":["FPGA"],"title":"MOCHA: Multinode Cost Optimization in Heterogeneous Clouds with Accelerators","type":"publication"},{"authors":["Michael Lo","Zhenman Fang","Jie Wang","Peipei Zhou","Mau-Chung Frank Chang","Jason Cong"],"categories":null,"content":"","date":1588318259,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588318259,"objectID":"b105445cfb4ab0be802768433894d8be","permalink":"https://peipeizhou-eecs.github.io/publication/algorithm-hardware-co-design-for-bqsr-acceleration-in-genome-analysis-toolkit/","publishdate":"2020-05-01T07:30:59.733Z","relpermalink":"/publication/algorithm-hardware-co-design-for-bqsr-acceleration-in-genome-analysis-toolkit/","section":"publication","summary":"Genome sequencing is one of the most important applications in healthcare and has a great potential to realize precision medicine and personalized healthcare. However, its computing process is very time consuming. Even pre-processing the raw sequence data of a whole genome for a single person to the analysis ready data can take several days on a single-core CPU.\n\nIn this paper, we propose to accelerate the performance of the widely used Genome Analysis ToolKit (GATK) using FPGAs. More specifically, we focus on the algorithm and hardware codesign for the Base Quality Score Re-calibration (BQSR) step in GATK, which is an important and time-consuming step to correct systematic errors made by a sequencing machine. Prior studies did not consider hardware acceleration for BQSR because it requires a large amount of memory with random access and has a lot of control flow. To address these challenges, we first adapt the algorithm to resolve the random memory access conflicts to achieve a fully pipelined accelerator design and reduce its dataset size. Second, we leverage the newly introduced large-capacity UltraRAM (URAM) in Xilinx UltraScale+ FPGAs to buffer BQSR‚Äôs large dataset on chip, and further optimize its operating frequency. Finally, we also explore the coarse-grained pipeline and parallelism to improve the overall performance of the BQSR accelerator. Compared to the latest software implementation of GATK 4.1 running on single-thread and 56-thread CPUs (14nm Xeon E5-2680 v4), our FPGA accelerator running on Xilinx 16nm UltraScale+ VCU1525 board achieves up to 40.7x and 8.5x speedups, respectively.","tags":["FCCM"],"title":"Algorithm-Hardware Co-design for BQSR Acceleration in Genome Analysis ToolKit","type":"publication"},{"authors":["Peipei Zhou"],"categories":null,"content":"","date":1560165499,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560165499,"objectID":"126abc48a4a1b5e5269b8f38857877be","permalink":"https://peipeizhou-eecs.github.io/publication/2019_phd/","publishdate":"2019-06-10T11:18:19.911Z","relpermalink":"/publication/2019_phd/","section":"publication","summary":"This dissertation investigates design target, modeling, and optimization for field-programmable gate array (FPGA) customized computing at chip-level, node-level and cluster-level. FPGAs have gained popularity in the acceleration of a wide range of applications with 10x-100x performance/energy efficiency over the general-purpose processors. The design choices of FPGA accelerators for different targets at different levels are enormous. To guide the designers to find the best design choices, modeling is inevitable. Chip-level performance and energy modeling for embedded and low-power devices. We first study the single chip performance and energy model for FPGA-based pipelined design. Customized pipeline designs that minimize the pipeline initiation interval (II) maximize the throughput of FPGA accelerators designed with high-level synthesis (HLS). However, II1 can reduce dynamic energy below II=1 due to interconnect savings. We use analytic models to describe accelerator performance and energy, explore the trade-offs of energy and accelerator performance. and find the energy optimal design point. Chip-level performance and frequency improvement through locality-aware transformation in HLS. We then study timing degradation problems in HLS-based accelerator design and classify four patterns: scatter, gather, broadcast, and reduce in the context of on-chip data movement. We observe that the on-chip data path delay in these patterns scales up when the design size increases, but HLS tools do not estimate the interconnect delay correctly or make a conscientious effort to control or cap the growth of long interconnect delays at the HLS level. We propose a Latte microarchitecture that features pipelined transfer controllers (PTC) to reduce critical path and improves timing by 1.50x on average. Node-level performance and cost modeling for FPGA-enabled, storage-optimized public cloud instances. At node level, We study performance and cost models for customized computing in light of the fact that performance and cost are primary concerns when deploying applications and services in a pay-as-you-go public cloud. The performance and cost modeling are discussed in two aspects, computation resources, with CPUs and locally PCIe-attached accelerators, and storage resources including SSDs and HDDs. For computation resources, improved performance using accelerators is accompanied by a higher cost per hour. We discuss the performance and cost modeling of deploying FPGA accelerators, offer insights on accelerator kernel design, and discuss when we should scale up by using FPGA in a node or by choosing a larger instance which has more CPU cores per node. For storage resources, storage systems (SSD/HDD) need to be carefully chosen to match the performance improvement introduced by accelerators while achieving the optimal cost. We conduct quantitative performance analysis on the Spark-based production-quality genome analysis toolkit. We then propose I/O-aware performance analysis and modeling for a broad set of Spark applications. Based on the model, we optimize the cost of genome sequencing in the public cloud by 38%, compared to a configuration recommended by the Spark Official website. Cluster-level performance and cost modeling for sharing FPGAs among different instances. From a node-level performance and cost model, we learn that simply offloading accelerated kernels from CPU hosts to PCIe-based FPGAs does not guarantee improvement in terms of out-of-pocket cost when using pay-as-you-go services in a public cloud. We analyze the application execution and conclude that the extra cost is attributable to insufficient application-level speedup by Amdahl‚Äôs law. To achieve cost saving with the use of FPGA accelerators in the public cloud, we propose to share one FPGA among multiple CPU instances when the number of CPU cores in one instance cannot fully utilize the FPGA accelerator computation resource. By implementing this idea, we present Mocha framework in this dissertation as a distributed runtime system to optimize the out-of-pocket cost while keeping high speedup and throughput. To demonstrate the performance improvement and cost saving of modeling in customized computing, we use genome pipeline optimization in the public cloud and private cloud as case studies showing how to conduct optimal scheduling under certain constraints. In the public cloud, where cost is the primary concern, we formulate how to select instances and schedule genome stages to achieve the least cost given certain deadline constraints as a MILP (mixed integer linear programming) problem. In a private cloud, where hardware (CPU cores, storage disks) is given, we formulate the scheduling of multiple genomes to achieve the least latency, as a MILP problem.","tags":["Thesis"],"title":"Modeling and Optimization for Customized Computing: Performance, Energy and Cost Perspective","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://peipeizhou-eecs.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Jason Cong","Peipei Zhou"],"categories":null,"content":"","date":1544444034,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544444034,"objectID":"7faed76bbfac0ea5c18f095d72a2b030","permalink":"https://peipeizhou-eecs.github.io/talk/customizable-domain-specific-computing/","publishdate":"2021-08-17T12:13:54.603Z","relpermalink":"/talk/customizable-domain-specific-computing/","section":"event","summary":"","tags":null,"title":"Customizable Domain Specific Computing","type":"event"},{"authors":["Chen Zhang","Guangyu Sun","Zhenman Fang","Peipei Zhou","Peichen Pan","Jason Cong"],"categories":null,"content":"","date":1542453237,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542453237,"objectID":"9a8adcaf1ff46aee8d1f93b9277d279f","permalink":"https://peipeizhou-eecs.github.io/publication/2018_tcad/","publishdate":"2018-11-17T11:13:57.518Z","relpermalink":"/publication/2018_tcad/","section":"publication","summary":"With the recent advancement of multilayer convolutional neural networks (CNN) and fully connected networks (FCN), deep learning has achieved amazing success in many areas, especially in visual content understanding and classification. To improve the performance and energy efficiency of the computation-demanding CNN, the FPGAbased acceleration emerges as one of the most attractive alternatives. In this paper we design and implement Caffeine, a hardware/ software co-designed library to efficiently accelerate the entire CNN and FCN on FPGAs. First, we propose a uniformed convolutional matrixmultiplication representation for both computation-bound convolutional layers and communication-bound fully connected (FCN) layers. Based on this representation, we optimize the accelerator micro-architecture and maximize the underlying FPGA computing and bandwidth resource utilization based on a revised roofline model. Moreover, we design an automation flow to directly compile high-level network definitions to the final FPGA accelerator. As a case study, we integrate Caffeine into the industry-standard software deep learning framework Caffe. We evaluate Caffeine and its integration with Caffe by implementing VGG16 and AlexNet networks on multiple FPGA platforms. Caffeine achieves a peak performance of 1,460 GOPS on a medium-sized Xilinx KU060 FPGA board; to our knowledge, this is the best published result. It achieves more than 100x speed-up on FCN layers over prior FPGA accelerators. An end-to-end evaluation with Caffe integration shows up to 29x and 150x performance and energy gains over Caffe on a 12-core Xeon server, and 5.7x better energy efficiency over the GPU implementation. Performance projections for a system with a high-end FPGA (Virtex7 690t) show even higher gains.","tags":["TCAD"],"title":"Caffeine: Towards Uniformed Representation and Acceleration for Deep Convolutional Neural Networks (üî•Best Paper)","type":"publication"},{"authors":["Yuze Chi","Jason Cong","Peng Wei","Peipei Zhou"],"categories":null,"content":"","date":1542444246,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542444246,"objectID":"f629e0aebdc547b4513b8e28e5851369","permalink":"https://peipeizhou-eecs.github.io/publication/2018_iccad/","publishdate":"2018-11-17T08:44:06.201Z","relpermalink":"/publication/2018_iccad/","section":"publication","summary":"Stencil computation is one of the most important kernels in many application domains such as image processing, solving partial diferential equations, and cellular automata. Many of the stencil kernels are complex, usually consist of multiple stages or iterations, and are often computation-bounded. Such kernels are often off-loaded to FPGAs to take advantages of the efficiency of dedicated hardware. However, implementing such complex kernels efficiently is not trivial, due to complicated data dependencies, difficulties of programming FPGAs with RTL, as well as large design space.\nIn this paper we present SODA, an automated framework for implementing Stencil algorithms with Optimized Datalow Architecture on FPGAs. The SODA microarchitecture minimizes the on-chip reuse bufer size required by full data reuse and provides flexible and scalable fine-grained parallelism. The SODA automation framework takes high-level user input and generates efficient, high-frequency datalow implementation. This significantly reduces the difficulty of programming FPGAs efficiently for stencil algorithms. The SODA design-space exploration framework models the resource constraints and searches for the performance-optimized coniguration with accurate models for post-synthesis resource utilization and on-board execution throughput. Experimental results from on-board execution using a wide range of benchmarks show up to 3.28x speed up over 24-thread CPU and our fully automated framework achieves better performance compared with manually designed state-of-the-art FPGA accelerators.","tags":["ICCAD"],"title":"SODA: Stencil with Optimized Dataflow Architecture (üî•Best Paper Nominee)","type":"publication"},{"authors":["Jason Cong","Zhenman Fang","Yuchen Hao","Peng Wei","Cody Hao Yu","Chen Zhang","Peipei Zhou"],"categories":null,"content":"","date":1534504545,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534504545,"objectID":"1835eafa9e829c1c9391c0ab5c2adfc9","permalink":"https://peipeizhou-eecs.github.io/publication/2018_arxiv/","publishdate":"2018-08-17T11:15:45.333Z","relpermalink":"/publication/2018_arxiv/","section":"publication","summary":"FPGA-based heterogeneous architectures provide programmers with the ability to customize their hardware accelerators for flexible acceleration of many workloads. Nonetheless, such advantages come at the cost of sacrificing programmability. FPGA vendors and researchers attempt to improve the programmability through high-level synthesis (HLS) technologies that can directly generate hardware circuits from high-level language descriptions. However, reading through recent publications on FPGA designs using HLS, one often gets the impression that FPGA programming is still hard in that it leaves programmers to explore a very large design space with many possible combinations of HLS optimization strategies. In this paper we make two important observations and contributions. First, we demonstrate a rather surprising result: FPGA programming can be made easy by following a simple best-effort guideline of five refinement steps using HLS. We show that for a broad class of accelerator benchmarks from MachSuite, the proposed best-effort guideline improves the FPGA accelerator performance by 42-29,030x. Compared to the baseline CPU performance, the FPGA accelerator performance is improved from an average 292.5x slowdown to an average 34.4x speedup. Moreover, we show that the refinement steps in the best-effort guideline, consisting of explicit data caching, customized pipelining, processing element duplication, computation/communication overlapping and scratchpad reorganization, correspond well to the best practice guidelines for multicore CPU programming. Although our best-effort guideline may not always lead to the optimal solution, it substantially simplifies the FPGA programming effort, and will greatly support the wide adoption of FPGA-based acceleration by the software programming community.","tags":["arXiv"],"title":"Best-effort FPGA programming: a few steps can go a long way","type":"publication"},{"authors":["Peipei Zhou","Zhenyuan Ruan","Zhenman Fang","Megan Shand","David Roazen","Jason Cong"],"categories":null,"content":"","date":1526641566,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526641566,"objectID":"94e18cfcb9e3c8806ad5a0e003c4a5e9","permalink":"https://peipeizhou-eecs.github.io/publication/2018_ispass/","publishdate":"2018-05-18T11:06:06.481Z","relpermalink":"/publication/2018_ispass/","section":"publication","summary":"In conventional Hadoop MapReduce applications, I/O used to play a heavy role in the overall system performance. More recently, a study from the Apache Spark community‚Äîstate-of-the-art in-memory cluster computing framework‚Äîreports that I/O is no longer the bottleneck and has a marginal performance impact on applications like SQL processing. However, we observe that simply replacing HDDs with SSDs in a Spark cluster can have over 10x performance improvement for certain stages in large-scale production-quality genome processing. Therefore, one key question arises: How does I/O quantitatively impact the performance of today‚Äôs big data applications developed using in-memory cluster computing frameworks like Apache Spark? In this paper we select an important yet complex application‚Äîthe Spark-based Genome Analysis ToolKit (GATK4)‚Äîto guide our modeling. We first use different combinations of HDDs and SSDs to measure the I/O impact on GATK4 and change the CPU core number to discover the relation between computation and I/O access. Combining with Spark underlying implementations, we further analyze the inherent cause of the above observations and build our model based on the analysis. Although building upon GATK4, our model maintains generality to other applications. Experimental results show that we can achieve an performance prediction error rate within 10% for typical Spark applications of both iterative and shuffle-heavy algorithms. Finally, we further extend our model to a broader area - that of optimal configuration selection in the public cloud. In Google Cloud, our model enables us to save 38% to 57% cost for genome sequencing compared with its recommended default configurations. Currently, more and more companies are adopting cloud computing for specific workloads. Our proposed model can have a huge impact on their choices, while also enabling them to significantly reduce their costs.","tags":["ISPASS"],"title":"Doppio: I/O-Aware Performance Analysis, Modeling and Optimization for In-Memory Computing Framework(üî•Best Paper Nominee)","type":"publication"},{"authors":["Jason Cong","Peng Wei","Cody Hao Yu","Peipei Zhou"],"categories":null,"content":"","date":1526548483,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526548483,"objectID":"9195139fb3f77652cda4d3cfa204de1e","permalink":"https://peipeizhou-eecs.github.io/publication/2018_fccm_latte/","publishdate":"2018-05-17T09:14:43.992Z","relpermalink":"/publication/2018_fccm_latte/","section":"publication","summary":"Modern FPGA chips feature abundant reconfigurable resources such as LUTs, FFs, BRAMs and DSPs. High-level synthesis (HLS) further advances users productivity in designing accelerators and scaling out the design quickly via fine-grain and coarse-grain pipelining and duplication to utilize on-chip resources. However, current HLS tools fail to consider data locality in the scaled-out design; this leads to a long critical path which results in a low operating frequency. In this paper we summarize the timing degradation problems to four common collective communication and computation patterns in HLS-based accelerator design: scatter, gather, broadcast and reduce. These widely used patterns scale poorly in one-to-all or all-to-one data movements between off-chip communication interface and on-chip storage, or inside the computation logic. Therefore, we propose the Latte microarchitecture featuring pipelined transfer controllers (PTC) along data paths in these patterns. Furthermore, we implement an automated framework to apply our Latte implementation in HLS with minimal user efforts. Our experiments show that Latte-optimized designs greatly improve the timing of baseline HLS designs by 1.50x with only 3.2% LUT overhead on average, and 2.66x with 2.7% overhead at maximum.","tags":["FCCM","HLS"],"title":"Latte: Locality Aware Transformation for High-Level Synthesis","type":"publication"},{"authors":["Zhenyuan Ruan","Tong He","Bojie Li","Peipei Zhou","Jason Cong"],"categories":null,"content":"","date":1526468642,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526468642,"objectID":"74cfa225d4a4adc3dcea7fb22d36f66c","permalink":"https://peipeizhou-eecs.github.io/publication/2018_fccm_staccel/","publishdate":"2018-05-16T11:04:02.914Z","relpermalink":"/publication/2018_fccm_staccel/","section":"publication","summary":"In recent years we have witnessed the emergence of the FPGA in many high-performance systems. This is due to FPGA's high reconfigurability and improved user-friendly programming environment. OpenCL, supported by major FPGA vendors, is a high-level programming platform that liberates hardware developers from having to deal with the complex and error-prone HDL development. While OpenCL exposes a GPU-like programming model, which is well-suited for compute-intensive tasks, in many state-of-art systems that deploy FPGA, we observe that the workloads are streaming-like, which is communication-intensive. This mismatch leads to low throughput and high end-to-end latency.\nIn this paper, we propose ST-Accel, a new high-level programming platform for streaming applications on FPGA. It has the following advantages: (i) ST-Accel adopts the multiprocessing programming model to capture the inherent pipeline-level parallelism of streaming applications while reducing the end-to-end latency. (ii) A message-passing-based host/FPGA communication model is used to avoid the coherency issue of shared memory, thus enabling host/FPGA communication during kernel execution. (iii) ST-Accel provides a high-level abstraction for I/O devices to support direct I/O device access that eliminates the overhead of host CPU and reduces the I/O latency. (iv) ST-Accel enables the decoupled access/execute architecture to maximize the utilization of I/O devices. (v) The host/FPGA communication interface is redesigned to cater to the demands of both latency-critical and throughput-critical scenarios. The experimental results on the Amazon AWS cloud and local machine show that ST-Accel can achieve 1.6X-166X throughput and 1/3 latency for typical streaming workloads when compared to OpenCL.","tags":["FCCM"],"title":"ST-Accel: A High-Level Programming Platform for Streaming Applications on FPGA","type":"publication"},{"authors":["Jason Cong","Peng Wei","Cody Hao Yu","Peipei Zhou"],"categories":null,"content":"","date":1497697632,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497697632,"objectID":"6f2042b6da9c691ff552e3eb3c4924d5","permalink":"https://peipeizhou-eecs.github.io/publication/2017_dac/","publishdate":"2017-06-17T11:07:12.497Z","relpermalink":"/publication/2017_dac/","section":"publication","summary":"High-level synthesis (HLS) is getting increasing attention from both academia and industry for high-quality and high-productivity designs. However, when inferring primitive-type arrays in HLS designs into on-chip memory buffers, commercial HLS tools fail to effectively organize FPGAs‚Äô on-chip BRAM building blocks to realize high-bandwidth data communication; this often leads to suboptimal quality of results. This paper addresses this issue via automated on-chip buffer restructuring. Specifically, we present three buffer restructuring approaches and develop an analytical model for each approach to capture its impact on performance and resource consumption. With the proposed model, we formulate the process of identifying the optimal design choice into an integer non-linear programming (INLP) problem and demonstrate that it can be solved efficiently with the help of a one-time C-to-HDL(hardware description language) synthesis. The experimental results show that our automated source-to-source code transformation tool improves the performance of a broad class of HLS designs by averagely 4.8x.\n","tags":["DAC"],"title":"Bandwidth Optimization Through On-Chip Memory Restructuring for HLS","type":"publication"},{"authors":["Chen Zhang","Zhenman Fang","Peipei Zhou","Peichen Pan","Jason Cong"],"categories":null,"content":"","date":1479380892,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479380892,"objectID":"8d9d492fc557a37c4e291e1130b39b4c","permalink":"https://peipeizhou-eecs.github.io/publication/2016_iccad/","publishdate":"2016-11-17T11:08:12.2Z","relpermalink":"/publication/2016_iccad/","section":"publication","summary":"With the recent advancement of multilayer convolutional neural networks (CNN), deep learning has achieved amazing success in many areas, especially in visual content understanding and classification. To improve the performance and energy-efficiency of the computation-demanding CNN, the FPGA-based acceleration emerges as one of the most attractive alternatives. In this paper we design and implement Caffeine, a hardware/soft-ware co-designed library to efficiently accelerate the entire CNN on FPGAs. First, we propose a uniformed convolutional matrix-multiplication representation for both computation-intensive con-volutional layers and communication-intensive fully connected (FCN) layers. Second, we design Caffeine with the goal to maximize the underlying FPGA computing and bandwidth resource utilization , with a key focus on the bandwidth optimization by the memory access reorganization not studied in prior work. Moreover , we implement Caffeine in the portable high-level synthesis and provide various hardware/software definable parameters for user configurations. Finally, we also integrate Caffeine into the industry-standard software deep learning framework Caffe. We evaluate Caffeine and its integration with Caffe by implementing VGG16 and AlexNet network on multiple FPGA platforms. Caffeine achieves a peak performance of 365 GOPS on Xilinx KU060 FPGA and 636 GOPS on Virtex7 690t FPGA. This is the best published result to our best knowledge. We achieve more than 100x speedup on FCN layers over previous FPGA accelerators. An end-to-end evaluation with Caffe integration shows up to 7.3x and 43.5x performance and energy gains over Caffe on a 12-core Xeon server, and 1.5x better energy-efficiency over the GPU implementation on a medium-sized FPGA (KU060). Performance projections to a system with a high-end FPGA (Virtex7 690t) shows even higher gains.\n","tags":["ICCAD"],"title":"Caffeine: Towards Uniformed Representation and Acceleration for Deep Convolutional Neural Networks","type":"publication"},{"authors":["Peipei Zhou","Hyunseok Park","Zhenman Fang","Jason Cong","Andr√© DeHon"],"categories":null,"content":"","date":1463483351,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1463483351,"objectID":"158c9f7932655f64a130d290c0922ca5","permalink":"https://peipeizhou-eecs.github.io/publication/2016_fccm/","publishdate":"2016-05-17T11:09:11.723Z","relpermalink":"/publication/2016_fccm/","section":"publication","summary":"Customized pipeline designs that minimize the pipeline initiation interval (II) maximize the throughput of FPGA accelerators designed with high-level synthesis (HLS). What is the impact of minimizing II on energy efficiency? Using a matrix-multiply accelerator, we show that matrix multiplies with II1 can sometimes reduce dynamic energy below II=1 due to interconnect savings, but II=1 always achieves energy close to the minimum. We also identify sources of inefficient mapping in the commercial tool flow. ","tags":["FCCM"],"title":"Energy Efficiency of Full Pipelining: A Case Study for Matrix Multiplication","type":"publication"},{"authors":["Yu-Ting Chen","Jason Cong","Zhenman Fang","Peipei Zhou"],"categories":null,"content":"","date":1455707410,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1455707410,"objectID":"33aa904ca59fdd7816ec87159b216c21","permalink":"https://peipeizhou-eecs.github.io/publication/2016_fpga/","publishdate":"2016-02-17T11:10:10.25Z","relpermalink":"/publication/2016_fpga/","section":"publication","summary":"Compared to conventional general-purpose processors, accelerator-rich architectures (ARAs) can provide orders-of-magnitude performance and energy gains. In this paper we design and implement the ARAPrototyper to enable rapid design space explorations for ARAs in real silicons and reduce the tedious prototyping efforts. First, ARAPrototyper provides a reusable baseline prototype with a highly customizable memory system, including interconnect between accelerators and buffers, interconnect between buffers and last-level cache (LLC) or DRAM, coherency choice at LLC or DRAM, and address translation support. To provide more insights into performance analysis, ARAPrototyper adds several performance counters on the accelerator side and leverages existing performance counters on the CPU side. Second, ARAPrototyper provides a clean interface to quickly integrate a user?s own accelerators written in high-level synthesis (HLS) code. Then, an ARA prototype can be automatically generated and mapped to a Xilinx Zynq SoC. To quickly develop applications that run seamlessly on the ARA prototype, ARAPrototyper provides a system software stack and abstracts the accelerators as software libraries for application developers. Our results demonstrate that ARAPrototyper enables a wide range of design space explorations for ARAs at manageable prototyping efforts and 4,000 to 10,000X faster evaluation time than full-system simulations. We believe that ARAPrototyper can be an attractive alternative for ARA design and evaluation.","tags":["FPGA"],"title":"ARAPrototyper: Enabling Rapid Prototyping and Evaluation for Accelerator-Rich Architecture","type":"publication"},{"authors":["Yu-Ting Chen","Jason Cong","Jie Lei","Sen Li","Myron Peto","Paul Spellman","Peng Wei","Peipei Zhou"],"categories":null,"content":"","date":1439809861,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1439809861,"objectID":"25c4eb18ecce8c7d14cb9bd78e5f1812","permalink":"https://peipeizhou-eecs.github.io/publication/2015_hitseq/","publishdate":"2015-08-17T11:11:01.653Z","relpermalink":"/publication/2015_hitseq/","section":"publication","summary":"The deep-coverage whole-genome sequencing (WGS) can generate billions of reads to be sequenced. It is time consuming for state-of-the-art aligners, such as BWA-MEM, to align the tremendous number of reads onto the reference genome. Inherently, the reads can be aligned using a massively parallel approach, and the alignment process should not be bounded by the limited number of computing cores of a single server. We present cloudscale BWAMEM (CS-BWAMEM), an ultrafast and highly scalable aligner built on top of cloud infrastructures. It leverages the abundant computing resources in a public or private cloud to fully exploit the parallelism obtained from the enormous number of reads. With CSBWAMEM, the pair-end whole-genome reads (30x) can be aligned within 80 minutes in a 25-node cluster with 300 cores.","tags":["HITSEQ"],"title":"CS-BWAMEM: A fast and scalable read aligner at the cloud scale for whole genome sequencing","type":"publication"},{"authors":["Peipei Zhou"],"categories":null,"content":"","date":1408274236,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1408274236,"objectID":"277960f972b2339393b18fea6ae0f21b","permalink":"https://peipeizhou-eecs.github.io/publication/2014_master/","publishdate":"2014-08-17T11:17:16.204Z","relpermalink":"/publication/2014_master/","section":"publication","summary":"Future processor will not be limited by the transistor resources, but will be mainly constrained by energy efficiency. Reconfigurable architecture offers higher energy efficiency than CPUs through customized hardware and more flexibility than ASICs. FPGAs allow configurability at bit level to keep both efficiency and flexibility. However, in many computation-intensive applications, only word level customizations are necessary, which inspires coarse-grained reconfigurable arrays(CGRAs) to raise configurability to word level and to reduce configuration information, and to enable on-the-fly customization. Traditional CGRAs are designed in the era when transistor resources are scarce. Previous work in CGRAs share hardware resources among different operations via modulo scheduling and time multiplexing processing elements. In the emerging scenario where transistor resources are rich, we develop a novel CGRA architecture that features full pipelining and dynamic composition to improve energy efficiency and implement the prototype on Xilinx Virtex-6 FPGA board. Experiments show that fully pipelined and dynamically composable architecture(FPCA) can exploit the energy benefits of customization for user applications when the transistor resources are rich.\n","tags":["Thesis"],"title":"A Fully Pipelined and Dynamically Composable Architecture of CGRA (Coarse Grained Reconfigurable Architecture)","type":"publication"},{"authors":["Jason Cong","Hui Huang","Chiyuan Ma","Bingjun Xiao","Peipei Zhou"],"categories":null,"content":"","date":1400325137,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1400325137,"objectID":"9713b3a38591e8db745ec82901bb828b","permalink":"https://peipeizhou-eecs.github.io/publication/2014_fccm/","publishdate":"2014-05-17T11:12:17.735Z","relpermalink":"/publication/2014_fccm/","section":"publication","summary":"Future processor chips will not be limited by the transistor resources, but will be mainly constrained by energy efficiency. Reconfigurable fabrics bring higher energy efficiency than CPUs via customized hardware that adapts to user applications. Among different reconfigurable fabrics, coarse-grained reconfigurable arrays (CGRAs) can be even more efficient than fine-grained FPGAs when bit-level customization is not necessary in target applications. CGRAs were originally developed in the era when transistor resources were more critical than energy efficiency. Previous work shares hardware among different operations via modulo scheduling and time multiplexing of processing elements. In this work, we focus on an emerging scenario where transistor resources are rich. We develop a novel CGRA architecture that enables full pipelining and dynamic composition to improve energy efficiency by taking full advantage of abundant transistors. Several new design challenges are solved. We implement a prototype of the proposed architecture in a commodity FPGA chip for verification. Experiments show that our architecture can fully exploit the energy benefits of customization for user applications in the scenario of rich transistor resources.","tags":["FCCM"],"title":"A Fully Pipelined and Dynamically Composable Architecture of CGRA","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://peipeizhou-eecs.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://peipeizhou-eecs.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]